<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Access Control on KL46Z</title>
    <url>/2025/01/07/Doorcontrol/</url>
    <content><![CDATA[<p><img src="/images/post1/3d.png" alt="cover"></p>
<p>A compact access-control unit based on KL46Z.</p>
<span id="more"></span>

<p>I built a compact access-control unit around the NXP FRDM-KL46Z (MKL46Z256VLL4). It takes passwords from a 3×4 matrix keypad, shows immediate feedback on a 4-digit segmented LCD (SLCD), and actuates a door strike through a relay driver. Users and administrators follow separate workflows, and credentials are stored in on-chip Flash so the device remains functional after resets or power loss. The more important goal, however, was not the feature list—it was to make the system behave like a deployable device: predictable interaction, safe defaults, and resilience designed into the hardware rather than “patched” in firmware.</p>
<h2 id="Workflow-as-a-contract-not-just-a-diagram"><a href="#Workflow-as-a-contract-not-just-a-diagram" class="headerlink" title="Workflow as a contract, not just a diagram"></a>Workflow as a contract, not just a diagram</h2><p>In embedded projects, the fastest way to create bugs is to treat the workflow as something you “explain afterwards.” I used the workflow as a behavioural contract and tried to keep the firmware honest to it. Password entry is explicitly terminated by ‘#’, so the user never wonders whether the device is still listening, and the device never has to guess when the input ends. Admin mode is deliberately harder to enter: a “hold ‘#’” gesture prevents accidental entry, and an admin password gate prevents destructive operations from being triggered by casual keypad use. Once inside admin mode, the actions are intentionally limited to what is operationally meaningful on a minimal UI: select a user slot, then edit or delete.</p>
<p>This structure matters because the interface is constrained. With only a keypad and a 4-digit display, ambiguity becomes visible immediately. The constraint is a design tool: it forces the interaction to be deterministic and pushes complexity away from the user and into a state machine that can be tested.</p>
<p><img src="/images/post1/Workflow.png" alt="Firmware workflow"></p>
<h2 id="Hardware-designing-for-power-events-and-physical-boundaries"><a href="#Hardware-designing-for-power-events-and-physical-boundaries" class="headerlink" title="Hardware: designing for power events and physical boundaries"></a>Hardware: designing for power events and physical boundaries</h2><p>Most “it works on the bench” prototypes quietly assume stable power and forgiving loads. A door controller is the opposite: it sits on a wall, experiences interruptions, and interfaces with inductive hardware. The hardware therefore had to be a resilience layer, not just a carrier for the MCU.</p>
<p>The schematic was organised into functional blocks—power supply, USB interface, LCD mapping, keypad header, crystal oscillator, relay control, and MCU core—so each boundary can be reasoned about independently. That separation was not just for readability; it was a way to force myself to ask the right questions per block. Where does noise enter? What fails first? What happens on partial power? If a fault occurs, does the system fail closed?</p>
<p><img src="/images/post1/sch.png" alt="Schematic overview"></p>
<h3 id="Power-architecture-I-learned-to-distrust-“software-managed-resilience”"><a href="#Power-architecture-I-learned-to-distrust-“software-managed-resilience”" class="headerlink" title="Power architecture: I learned to distrust “software-managed resilience”"></a>Power architecture: I learned to distrust “software-managed resilience”</h3><p>Power ended up being the most instructive part of the build. The common temptation is to detect outages in firmware and then “switch to battery.” The problem is that the MCU is least trustworthy exactly when you need it to act—during a supply collapse and potential brownout. I therefore chose a hardware-defined source selection path (diode OR-ing) so AC-derived 5 V, USB 5 V, and a Li-ion backup path can coexist and the board naturally runs from the highest available source. Charging is handled by a dedicated IC so the battery remains ready without requiring any application-level control.</p>
<p>This is deliberately conservative. It is not the most efficiency-optimised solution, but it avoids subtle timing assumptions and reduces the risk of oscillating brownouts or undefined intermediate states. In other words, it makes power behaviour boring and predictable—which is exactly what firmware wants.</p>
<h3 id="Keypad-and-LCD-minimal-UI-forces-high-signal-to-noise-decisions"><a href="#Keypad-and-LCD-minimal-UI-forces-high-signal-to-noise-decisions" class="headerlink" title="Keypad and LCD: minimal UI forces high signal-to-noise decisions"></a>Keypad and LCD: minimal UI forces high signal-to-noise decisions</h3><p>The keypad interface is a classic embedded reliability trap. Without disciplined scanning and debounce, the system either misses presses or double-counts them, and users blame the device rather than the code. Here the hardware is simple (a matrix keypad header) and the reliability comes from the firmware producing stable key events. The LCD mapping uses the KL46Z SLCD capability so the CPU does not need to bit-bang segments. With only four digits, the display strategy must be restrained: show the most recent digits during entry, and keep prompts short and consistent rather than trying to “explain everything” on-screen.</p>
<h3 id="Relay-driver-actuation-is-a-safety-boundary"><a href="#Relay-driver-actuation-is-a-safety-boundary" class="headerlink" title="Relay driver: actuation is a safety boundary"></a>Relay driver: actuation is a safety boundary</h3><p>Unlocking is not just another GPIO toggle; it is a physical action that must be explicit, bounded, and electrically well-behaved. The relay is driven via a transistor stage sized for saturation, with a flyback diode to clamp inductive transients. The goal is not only to switch the load reliably, but also to ensure that switching does not inject disturbances into the logic supply that could cause resets or undefined states. A subtle but important mindset shift here is treating the relay path as part of system integrity: if actuation destabilises the MCU, then “unlock” becomes a denial-of-service vector and a debugging nightmare.</p>
<h2 id="PCB-when-the-design-becomes-real"><a href="#PCB-when-the-design-becomes-real" class="headerlink" title="PCB: when the design becomes real"></a>PCB: when the design becomes real</h2><p>A schematic that works is not yet a board that behaves. The PCB layout forces you to confront what matters physically: where current actually flows, how return paths behave, how connectors will be used, and how much “engineering intent” survives the move from a tidy diagram to copper.</p>
<p>I tried to keep the layout aligned with the functional boundaries: power and high-current paths are compact, the MCU and decoupling are treated as the stability core, and headers are placed so wiring is straightforward in an enclosure. Even on a small two-layer board, these decisions dominate robustness far more than any single clever circuit trick. The layout also makes it easier to reason about failures: when something goes wrong, you can localise it to a block instead of hunting across an entangled board.</p>
<p><img src="/images/post1/pcb.png" alt="PCB layout"></p>
<h2 id="3D-sanity-check-the-hidden-part-of-“deployable”"><a href="#3D-sanity-check-the-hidden-part-of-“deployable”" class="headerlink" title="3D sanity check: the hidden part of “deployable”"></a>3D sanity check: the hidden part of “deployable”</h2><p>The 3D render is not an aesthetic add-on; it is an engineering tool. It catches problems that are expensive to discover late: connector clearance, component height conflicts, and the practical alignment between the display, keypad cable, and enclosure openings. This is the stage where the project stops being an electronics exercise and starts resembling a small product. If the board cannot be assembled cleanly, mounted reliably, and connected without strain, then it is not deployable no matter how correct the firmware is.</p>
<h2 id="Persistence-Flash-is-not-a-database-so-treat-it-like-a-failure-surface"><a href="#Persistence-Flash-is-not-a-database-so-treat-it-like-a-failure-surface" class="headerlink" title="Persistence: Flash is not a database, so treat it like a failure surface"></a>Persistence: Flash is not a database, so treat it like a failure surface</h2><p>Storing credentials in Flash sounds simple until you consider partial writes. Power can fail mid-update, and Flash does not provide transactional semantics by default. The key design choice was to validate stored data before trusting it and to prefer safe rejection over optimistic acceptance. For access control, false negatives are annoying; false positives are unacceptable. That asymmetry shapes the storage logic and reinforces the broader theme: when uncertain, fail closed.</p>
<h2 id="What-I-would-improve-next-and-why"><a href="#What-I-would-improve-next-and-why" class="headerlink" title="What I would improve next (and why)"></a>What I would improve next (and why)</h2><p>If I iterate a second revision, I would start with boundary hardening rather than new features. Electrically, I would strengthen safety separation around the mains-derived supply and improve manufacturability with clearer test points and bring-up hooks. System-wise, I would formalise a low-power story so that battery-backed operation is not just “it stays on,” but “it stays on predictably for a quantified duration.” These are not headline-grabbing changes, but they are exactly what turns a working prototype into something trustworthy in day-to-day use.</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>Source code are available here: <a href="https://github.com/0xCapy/DoorAccessControl">https://github.com/0xCapy/DoorAccessControl</a></p>
]]></content>
      <categories>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>Embedded</tag>
        <tag>Control</tag>
        <tag>Firmware</tag>
      </tags>
  </entry>
  <entry>
    <title>hybrid PI–Residual RL: A Small, Explainable Patch on Top of PI</title>
    <url>/2025/11/05/UAV/</url>
    <content><![CDATA[<p><img src="/images/post2/flowchart.png" alt="Hybrid PI–RL structure"></p>
<p>A RL mixed solution improving PI stability for UAV motors.</p>
<span id="more"></span>

<p>This post is a project log of something I personally find more useful than “end-to-end RL replaces classical control”: I kept a conventional PI loop as the stable, interpretable baseline, and added a <strong>small, bounded residual correction</strong> on top of it. The goal was not to build a huge learning system, but to get a controller that behaves better under realistic nuisances—battery voltage sag, aerodynamic gust disturbances, and ESC-like actuation constraints—while still looking like something I would actually deploy.</p>
<p>The intuition is simple. PI is already doing most of the job. What it often lacks is a lightweight, context-aware correction when the operating conditions shift. Instead of rewriting the controller, I tried to add the missing “last mile” with a residual term that is deliberately limited in magnitude and filtered for smoothness.</p>
<h2 id="Why-a-hybrid-controller-instead-of-pure-neural-RL"><a href="#Why-a-hybrid-controller-instead-of-pure-neural-RL" class="headerlink" title="Why a hybrid controller instead of pure neural RL"></a>Why a hybrid controller instead of pure neural RL</h2><p>I wanted a controller whose behavior I can reason about and whose failure modes are not mysterious. In UAV motor speed control, the plant is not only nonlinear, but also constrained by very practical effects: duty saturation, duty slew limits, command delay, supply-voltage variations, and short gust-like load disturbances. These are exactly the kinds of details that can make a purely data-driven, high-capacity policy feel overpowered and under-explainable for the problem.</p>
<p>So I treated learning as a controlled add-on. PI remains the main feedback structure. The learned component is only allowed to make a small correction. This gives a clean engineering story: the baseline loop dynamics are preserved, while the residual absorbs some modeling mismatch and disturbance-induced transient issues.</p>
<h2 id="The-setup-make-“realism”-part-of-the-simulation"><a href="#The-setup-make-“realism”-part-of-the-simulation" class="headerlink" title="The setup: make “realism” part of the simulation"></a>The setup: make “realism” part of the simulation</h2><p>The simulation includes (i) a supply-voltage sag model to reflect a 3S battery droop over short horizons, (ii) a gust disturbance modeled as a short Gaussian torque pulse, and (iii) ESC-like actuation constraints: PWM duty in $[0,1]$, a duty rate limit, and a fixed command delay. The point is not to perfectly model every electrochemical detail, but to ensure the controller is trained and evaluated in conditions that resemble how a motor actually feels inside a UAV.</p>
<p>The reference is intentionally hybrid (ramp + steps + sinusoids) so the controller is forced to handle startup, tracking, and small oscillatory components in a single episode, while the gust creates an obvious transient “stress test” segment.</p>
<h2 id="Control-structure-PI-baseline-bounded-linear-residual"><a href="#Control-structure-PI-baseline-bounded-linear-residual" class="headerlink" title="Control structure: PI baseline + bounded linear residual"></a>Control structure: PI baseline + bounded linear residual</h2><p>The controller can be summarized as:</p>
<p>$$<br>d(t) &#x3D; \mathrm{sat}\big(d_{\mathrm{PI}}(t) + \Delta d(s),, 0,, 1\big)<br>$$</p>
<p>The residual policy is deliberately simple and explainable: it is a <strong>linear mapping</strong> from a normalized state vector to a duty-cycle correction, followed by a hard clip:</p>
<p>$$<br>\Delta d(s) &#x3D; \mathrm{clip}(\theta^\top s,,-a_{\max},,a_{\max})<br>$$</p>
<p>The state vector contains the instantaneous speed error, the integral error, the measured motor current, and the supply voltage. The residual output is additionally low-pass filtered before being applied, which reduces high-frequency switching and keeps the correction “well-behaved” even when the PI loop is already close to steady state.</p>
<p>The key design choice is that the residual is not allowed to dominate. With a small correction bound, saturation, and actuator constraints, the residual behaves like a patch—not a second controller fighting the first one.</p>
<h2 id="Training-without-drama-random-search-and-multi-objective-selection"><a href="#Training-without-drama-random-search-and-multi-objective-selection" class="headerlink" title="Training without drama: random search and multi-objective selection"></a>Training without drama: random search and multi-objective selection</h2><p>Because the system includes saturation, slew limiting, and delay, I avoided making this a “big RL pipeline” problem. Instead, I optimized the residual parameters $\theta$ offline using a gradient-free random search: propose candidate parameters, simulate a full episode, score performance, and iterate.</p>
<p>The objective is multi-criteria by design. Each rollout logs:</p>
<p>$$<br>\text{Tracking} \rightarrow \text{ITAE}, \qquad<br>\text{Energy} \rightarrow E_{\mathrm{Wh}}, \qquad<br>\text{Smoothness} \rightarrow \text{BandPow}_{lf}<br>$$</p>
<p>Rather than forcing a single weighted sum to decide everything, I treated the results as a Pareto trade-off surface and used hypervolume-based selection to identify a balanced controller. This matters because it is easy to reduce ITAE further by becoming more aggressive, but doing so can increase current draw and degrade smoothness without meaningful steady-state benefits.</p>
<h2 id="Results-better-transient-behavior-where-it-matters"><a href="#Results-better-transient-behavior-where-it-matters" class="headerlink" title="Results: better transient behavior where it matters"></a>Results: better transient behavior where it matters</h2><p>The plot below compares the reference, the pure PI baseline, and the hybrid controller. The “gust region” is the part that tends to expose whether a controller can reject disturbances cleanly without creating long, low-frequency oscillations. What I wanted from the residual is not magical tracking in every segment, but a more controlled transient: less lingering oscillation and a more decisive recovery.</p>
<p><img src="/images/post2/Compare.png" alt="Comparison of Baseline and Hybrid Controllers"></p>
<p>Quantitatively, the knee-point hybrid setting improves ITAE and the low-frequency band-power metric substantially compared to the PI baseline, with a small energy trade-off. In my run, ITAE drops from 12.427 to 5.628 and $\text{BandPow}_{lf}$ drops from 731.873 to 281.221, while energy increases from 0.030 Wh to 0.032 Wh. This is the kind of exchange I consider “engineering plausible”: a mild energy increase to buy a much cleaner dynamic response.</p>
<h2 id="Why-I-picked-the-knee-point-and-not-the-“best-ITAE”-point"><a href="#Why-I-picked-the-knee-point-and-not-the-“best-ITAE”-point" class="headerlink" title="Why I picked the knee point (and not the “best ITAE” point)"></a>Why I picked the knee point (and not the “best ITAE” point)</h2><p>Once you see the Pareto front, it becomes obvious that chasing only accuracy is not free. You can push ITAE down further, but you start paying with slightly higher energy and, depending on the setting, less desirable smoothness. In other words, past a certain point the marginal gain in tracking is not worth the added aggressiveness.</p>
<p>The knee point is a pragmatic default. It is not the extreme “accuracy-only” controller, and it is not the “energy-only” controller. It is the place where you still get most of the tracking and smoothness benefits without taking the full cost.</p>
<p><img src="/images/post2/Pareto.png" alt="Trade-off Between Tracking Accuracy, Energy, and Smoothness"></p>
<h2 id="Why-this-looks-deployable-to-me"><a href="#Why-this-looks-deployable-to-me" class="headerlink" title="Why this looks deployable (to me)"></a>Why this looks deployable (to me)</h2><p>The online computation is tiny: a dot product, a clip, and a first-order low-pass filter. There is no neural network inference and no complex memory state to manage. More importantly, the structure stays interpretable. If the controller changes behavior, there is a concise place to look: the residual weights $\theta$, and the state channels that drive them.</p>
<p>The safety story is also straightforward: the residual amplitude is bounded; the final duty is saturated; the command is rate-limited; and the actuation delay is explicitly modeled. In practical terms, the residual is prevented from “doing something clever” that a real ESC could never execute.</p>
<h2 id="Reproducibility-notes-and-next-steps"><a href="#Reproducibility-notes-and-next-steps" class="headerlink" title="Reproducibility notes and next steps"></a>Reproducibility notes and next steps</h2><p>Each episode runs for 1.0 s with sampling interval $dt &#x3D; 2 \times 10^{-4}$ s. The duty rate limit is $8~\mathrm{s}^{-1}$, the ESC command delay is 2 ms, the residual clip is $\pm 0.05$, and the residual low-pass coefficient is $\alpha &#x3D; 0.6$. PI gains are fixed; only $\theta$ is optimized offline.</p>
<p>Next, I want to move beyond a single-motor setup. Multi-motor propulsion introduces coupling and shared supply dynamics that could make the residual either more valuable or more sensitive. I also want to test the same hybrid idea under sensor noise, quantization, and communication delays, because those are the details that tend to decide whether a “good simulation controller” becomes a practical controller.</p>
<p>If you are also interested in the “classical control + tiny learning patch” direction, I think this is a promising pattern: keep the proven feedback structure, and use learning only where it has the highest leverage.</p>
]]></content>
      <categories>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>Control</tag>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/12/21/test/</url>
    <content><![CDATA[<p>Here is nothing</p>
<span id="more"></span>]]></content>
      <tags>
        <tag>Test</tag>
      </tags>
  </entry>
  <entry>
    <title>Pipelined INT8 Vector MAC on FPGA</title>
    <url>/2025/05/20/mul/</url>
    <content><![CDATA[<p style="text-align:center;">
  <img src="/images/post3/wallace_cover.png" alt="Wallace tree reduction sketch" style="width:40%; height:70%;" />
</p>
Wallace Tree, Scaling, and the Real Cost of “Fast”

<span id="more"></span>




<p>At a glance, the Wallace tree looks like an elegant hack: take a tall matrix of partial products and squash it into two rows quickly, then finish with a normal carry-propagate adder. What becomes more interesting in practice is that Wallace is not simply “faster multiplication”. It is a statement about where you want complexity to live. Instead of letting carries ripple all the way across at every intermediate addition, you keep carries local by using carry-save compression (3:2 compressors, i.e., full adders) until the very end. In other words, Wallace postpones global carry resolution and spends area and routing to buy down logic depth. On an FPGA, that trade is never purely logical; it is also physical. The structure you choose shapes the placement and wiring, which can matter as much as the gate-level critical path.</p>
<p style="text-align:center;">
  <img src="/images/post3/str.png" alt="pipline stage" style="width:70%; height:auto;" />
</p>

<h3 id="Worst-case-sizing-as-a-first-class-constraint"><a href="#Worst-case-sizing-as-a-first-class-constraint" class="headerlink" title="Worst-case sizing as a first-class constraint"></a>Worst-case sizing as a first-class constraint</h3><p>Before discussing clocks and lanes, the arithmetic bound has to be explicit. With unsigned INT8 operands, the maximum product is</p>
<p>$$<br>P_{\max} &#x3D; 255 \times 255 &#x3D; 65025 &#x3D; 2^{16} - 511.<br>$$</p>
<p>This single number governs the widths of the multiplier output, the adder tree, and the accumulator. The objective here is not to minimise bitwidth at all costs; it is to choose widths that are safe, structurally consistent across $N$, and convenient for pipelining.</p>
<div class="table-wrap">

<table>
<thead>
<tr>
<th>Block</th>
<th>Operand range</th>
<th align="right">Max. output</th>
<th align="right">Chosen width</th>
<th align="right">Latency</th>
</tr>
</thead>
<tbody><tr>
<td>8×8 Wallace multiplier</td>
<td>$(0 \ldots 255)$</td>
<td align="right">$(255^2 &#x3D; 65025)$</td>
<td align="right">17-bit</td>
<td align="right">3 stages</td>
</tr>
<tr>
<td>Adder tree $(N &#x3D; 1\text{–}16)$</td>
<td>$(N)$ products</td>
<td align="right">$(N \times 65025)$</td>
<td align="right">20 bit (at $N&#x3D;16$)</td>
<td align="right">$\max(2,\lceil \log_2 N \rceil)$</td>
</tr>
<tr>
<td>Accumulator (up to 1000 beats)</td>
<td>dot products</td>
<td align="right">$(1000 \times 65025)$</td>
<td align="right">32-bit</td>
<td align="right">1 cycle</td>
</tr>
</tbody></table>
</div>

<p>There are three decisions embedded in Table I that are easy to miss if you only look at the final RTL. First, the multiplier output is carried on 17 bits. While $65025$ fits in 16 bits, the additional guard bit makes the reduction and final CPA less fragile, especially when the multiplier is deeply pipelined and internal carries are being staged. Second, the adder tree width is driven by the worst-case merge of $N$ products, with the upper bound</p>
<p>$$<br>S_{\max}(N) &#x3D; N \cdot 65025,<br>$$</p>
<p>and the tree depth is treated as a structural parameter rather than an afterthought,</p>
<p>$$<br>L_{\text{tree}}(N) &#x3D; \max(2,\lceil \log_2 N \rceil).<br>$$</p>
<p>Third, the accumulator is kept at 32 bits even though $1000 \times 65025 &#x3D; 65{,}025{,}000$ would fit in fewer bits. On FPGA, 32-bit arithmetic is a comfortable “native” size that simplifies integration, and it gives headroom for extending the design (wider operands, higher beat counts) without reopening every bound.</p>
<h3 id="What-the-Wallace-tree-is-actually-buying"><a href="#What-the-Wallace-tree-is-actually-buying" class="headerlink" title="What the Wallace tree is actually buying"></a>What the Wallace tree is actually buying</h3><p>The Wallace tree is best described as a parallel reduction scheme for partial products. An $8\times 8$ multiplier generates eight shifted rows of partial products. If you add them naively, you either do it sequentially (shift-add over multiple cycles) or you build an array-like reduction where carries propagate repeatedly across the width. Wallace changes the game by using 3:2 compression to reduce the height of the matrix quickly without propagating carries globally at each step.</p>
<p>The practical effect is that the “hard” part of the computation is reorganised into a sequence of local operations. Each compression layer shortens the vertical dimension of the bit matrix and only emits carries to the next higher-weight column. This is why Wallace trees pair naturally with pipelining: you can register between compression layers and keep the combinational depth under control. The final CPA is the only place where full carry propagation is allowed to happen.</p>
<p>The less romantic part is that on FPGA, the “local” nature of compression is not guaranteed to be local in placement. A Wallace tree produces a dense pattern of adders with a wide fan-in&#x2F;fan-out across adjacent bit columns. With small widths like 8×8 this is manageable, but once you replicate lanes and add a pipelined adder tree on top, routing becomes a real cost centre. This is the main reason I did not treat “Wallace tree &#x3D; fastest” as a blanket truth; instead, I treated it as a design choice whose value must be validated post-implementation, not just post-synthesis.</p>
<h3 id="Pipeline-structure-and-a-latency-model-that-stays-simple"><a href="#Pipeline-structure-and-a-latency-model-that-stays-simple" class="headerlink" title="Pipeline structure and a latency model that stays simple"></a>Pipeline structure and a latency model that stays simple</h3><p>The multiplier is implemented as a three-stage pipelined Wallace reduction, followed by a final carry-propagate addition. Lane merging is done with a registered binary adder tree whose depth scales with $\log_2 N$. The accumulator adds one more registered stage under beat control. With this structure, the total latency remains predictable and the initiation interval stays at one cycle once the pipeline is filled. A simple mental model for the end-to-end latency in cycles is</p>
<p>$$<br>L_{\text{total}}(N) \approx 3 + \log_2(N) + 1,<br>$$</p>
<p>where the first term is the multiplier pipeline depth, the second term is the merging tree depth, and the final term is the accumulator stage. This is not meant to be a formal proof; it is meant to keep the design composable. When $N$ changes, the story does not change, only the constants do.</p>
<h3 id="Post-implementation-results-where-scaling-is-“worth-it”"><a href="#Post-implementation-results-where-scaling-is-“worth-it”" class="headerlink" title="Post-implementation results: where scaling is “worth it”"></a>Post-implementation results: where scaling is “worth it”</h3><p>After implementation, I compared configurations using an effective throughput definition</p>
<p>$$<br>TP &#x3D; \text{Lanes} \times F_{\max},<br>$$</p>
<p>and then normalised it both by area (CLBeq) and by power. The point here is to make scaling effects visible. If $N$ doubles but $F_{\max}$ collapses, the architectural decision is not actually buying throughput.</p>
<div class="table-wrap">

<table>
<thead>
<tr>
<th>Config</th>
<th align="right">Latency [ns]</th>
<th align="right">CLBeq</th>
<th align="right">$F_{\max}$ [MHz]</th>
<th align="right">Power [W]</th>
<th align="right">$TP$</th>
<th align="right">$TP&#x2F;\text{CLB}$</th>
<th align="right">$TP&#x2F;W$</th>
</tr>
</thead>
<tbody><tr>
<td>1×8×8</td>
<td align="right">4.736</td>
<td align="right">39.6</td>
<td align="right">211</td>
<td align="right">0.010</td>
<td align="right">211</td>
<td align="right">5.33</td>
<td align="right">21.1</td>
</tr>
<tr>
<td>4×8×8</td>
<td align="right">4.603</td>
<td align="right">133.85</td>
<td align="right">217</td>
<td align="right">0.023</td>
<td align="right">868</td>
<td align="right">6.49</td>
<td align="right">37.7</td>
</tr>
<tr>
<td>8×8×8</td>
<td align="right">4.780</td>
<td align="right">285.85</td>
<td align="right">209</td>
<td align="right">0.042</td>
<td align="right">1672</td>
<td align="right">6.23</td>
<td align="right">39.8</td>
</tr>
</tbody></table>
</div>

<p>The most useful conclusion from Table III is that scaling is not monotonic in “goodness”. The 4-lane configuration delivers the best throughput per area, which is a strong indicator that it sits near the efficient knee of this design’s scaling curve. The 8-lane configuration leads throughput per watt, which is consistent with amortising shared overheads, but it gives up some area efficiency, and it is also the most exposed to routing pressure. That observation is aligned with the earlier Wallace-tree reflection: once the design becomes physically dense, timing and power are no longer driven only by logical depth. They are driven by how hard the placer&#x2F;router has to work to keep high-activity signals short and well-clustered.</p>
<div style="display:flex; gap:16px; align-items:flex-start; flex-wrap:wrap;">
  <img src="/images/post3/eff.png" alt="Efficiency vs area (TP/CLB)" style="flex:1 1 360px; max-width:48%; height:auto;" />
  <img src="/images/post3/pow.png" alt="Power vs area (TP/W)" style="flex:1 1 360px; max-width:48%; height:auto;" />
</div>



<h3 id="A-more-honest-takeaway-on-Wallace-trees"><a href="#A-more-honest-takeaway-on-Wallace-trees" class="headerlink" title="A more honest takeaway on Wallace trees"></a>A more honest takeaway on Wallace trees</h3><p>If I had to compress the Wallace-tree lesson into one sentence, it would be this: Wallace is a technique for trading carry propagation depth for compressor density, and on FPGA that trade is mediated by routing. In small multipliers, Wallace often “just works” and gives you a clean pipelinable core. In a scalable lane-based design, Wallace becomes one of several interacting structures that collectively decide whether the design is compute-bound or wire-bound.</p>
<p>In this project, pipelining the Wallace reduction into three stages was not merely about increasing $F_{\max}$. It was about making the multiplier’s internal carry-save representation explicit and stageable, so that the rest of the system—lane replication, tree merging, accumulation—can be reasoned about with stable interfaces. The bigger win is architectural hygiene: once the multiplier is a well-behaved, latency-known component, the rest of the datapath can be tuned and scaled without repeatedly re-solving the same timing puzzle.</p>
<p>If I extend this work, the most interesting direction is not to add more lanes blindly, but to treat physical locality as a design parameter: clustering lanes, shaping the adder tree to match placement, and possibly exploring alternative compressor schedules that map more naturally onto FPGA fast-carry resources. That is the point where “multiplier architecture” stops being an isolated block choice and starts being full-stack hardware design.</p>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>Source code available: <a href="https://github.com/0xCapy/Vector-multiplier.git">https://github.com/0xCapy/Vector-multiplier.git</a> </p>
<hr>
]]></content>
      <categories>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>FPGA</tag>
        <tag>Verilog</tag>
      </tags>
  </entry>
</search>
